{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a4847e0c-f5eb-4c3b-bb15-e8ccc2b22e85",
   "metadata": {},
   "source": [
    "What else can be included\n",
    "\n",
    "- What is a foundation model and how to adapt it\n",
    "- What is an LLM\n",
    "- Type of transformers and architecture\n",
    "- Type of models (based on datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a9127c6-8198-4017-9d25-df917a4e5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8fa88a-edac-44e2-bc32-c96543f42b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5849e63-35d1-4834-abb0-6f502554a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7039af6b-f4db-42ba-ba9a-40f482422b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc01acd-324d-469b-b326-b61b05dd1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT = \"\"\"Qwen2 is the new series of Qwen large language models. For Qwen2, we release a number of base language models and \n",
    "instruction-tuned language models ranging from 0.5 to 72 billion parameters, including a Mixture-of-Experts model. This repo contains the \n",
    "instruction-tuned 1.5B Qwen2 model.\n",
    "\n",
    "Compared with the state-of-the-art opensource language models, including the previous released Qwen1.5, Qwen2 has generally surpassed most \n",
    "opensource models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language \n",
    "understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e1ef78-63c7-4bb6-bffd-d48da1479460",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_val = tokenizer.encode(INPUT_TEXT, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d5bc5bf-b7eb-4b9b-b767-d785a00c3f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{48: 'Q', 16948: 'wen', 17: '2', 374: ' is', 279: ' the', 501: ' new', 4013: ' series', 315: ' of', 1207: ' Q', 3460: ' large', 4128: ' language', 4119: ' models', 13: '.', 1752: ' For', 11: ',', 582: ' we', 4879: ' release', 264: ' a', 1372: ' number', 2331: ' base', 323: ' and', 715: ' \\n', 54974: 'instruction', 2385: '-t', 48883: 'uned', 23994: ' ranging', 504: ' from', 220: ' ', 15: '0', 20: '5', 311: ' to', 22: '7', 7094: ' billion', 5029: ' parameters', 2670: ' including', 386: ' M', 12735: 'ixture', 8668: '-of', 12: '-', 86141: 'Experts', 1614: ' model', 1096: ' This', 15867: ' repo', 5610: ' contains', 16: '1', 33: 'B', 382: '.\\n\\n', 1092: 'Com', 7212: 'pared', 448: ' with', 1584: ' state', 10603: '-the', 37821: '-art', 15885: ' opens', 919: 'ource', 3681: ' previous', 5880: ' released', 702: ' has', 8789: ' generally', 67228: ' surpassed', 1429: ' most', 44408: 'opensource', 20459: ' demonstrated', 76551: ' competitiveness', 2348: ' against', 33233: ' proprietary', 3941: ' across', 62019: ' benchmarks', 24132: ' targeting', 369: ' for', 7995: 'under', 10070: 'standing', 9471: ' generation', 2745: ' mult', 49823: 'ilingual', 22302: ' capability', 10822: ' coding', 37596: ' mathematics', 32711: ' reasoning', 4992: ' etc'}\n"
     ]
    }
   ],
   "source": [
    "# Lets see how the qwen2 model encode the value\n",
    "enc_dec_dict = {}\n",
    "for num in encoded_val[0]:\n",
    "    dec_val = tokenizer.decode(num)\n",
    "    enc_dec_dict[num.item()] = dec_val\n",
    "\n",
    "print(enc_dec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0cbad3f-1200-44cf-a8f2-7c85181d6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_val = tokenizer.decode(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d61c1782-9be7-4b5c-b646-175bad9b48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets calculate the probability of next token\n",
    "inputs = encoded_val.to(DEVICE)\n",
    "with torch.no_grad(): # disabling gradient calculation to save memory, as its not required during inference\n",
    "    logits = model(inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f177a909-f512-4167-8985-f35fb1d501e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf09a773-4cb1-4774-994e-e3927d95eb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 140, 151936])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here the logits contains a nested list with 140 tokens and the entire vocabulary of the model\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c32fd03-5d75-459d-9ec5-c8185347cd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 151646)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the vocabulary\n",
    "\n",
    "vocabulary = tokenizer.get_vocab()\n",
    "type(vocabulary), len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11d48a52-820b-4a07-a94f-612160fbf9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "('#', 2)\n",
      "('$', 3)\n",
      "('%', 4)\n",
      "('&', 5)\n",
      "(\"'\", 6)\n",
      "('(', 7)\n",
      "(')', 8)\n",
      "('*', 9)\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "vocabulary_sorted = sorted(vocabulary.items(), key=lambda kv: kv[1])\n",
    "for i in vocabulary_sorted[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54959a8d-9eac-4afe-a83c-7cd9af68d3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RESET', 50000)\n",
      "('Ġpostpon', 50001)\n",
      "('Discover', 50002)\n",
      "('arrison', 50003)\n",
      "('shaw', 50004)\n",
      "('blood', 50005)\n",
      "('AJOR', 50006)\n",
      "('æĽ´æĸ°', 50007)\n",
      "('ĠMuse', 50008)\n",
      "('æĶ¶', 50009)\n"
     ]
    }
   ],
   "source": [
    "for i in vocabulary_sorted[50000:50010]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef211234-163e-4722-95b0-67c661f1d968",
   "metadata": {},
   "source": [
    "So the tokenizer is doing nothing, but breaking the text into tokens and then mapping into the Ids present in the models vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9683867-5703-49f5-8bff-c6dc4b002bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 140, 151936])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12599b-c5d2-4b0d-bd07-816c228445e9",
   "metadata": {},
   "source": [
    "This is nothing but a matrix, where\n",
    "\n",
    "number of batch = 1 <br>\n",
    "input tokens = 140 <br>\n",
    "vocabulary size = 151936 <br>\n",
    "\n",
    "If we see this as a table (n-gram table), the index will be the input ids and the columns is the vocabulary (starting from 0 to MAX). And the corresponding shell value is the logit value of the token (column) for the respective token Ids. Lets get the value after the last token and its probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a5743bd-cfe9-4313-96df-cbaa7b6d7c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = logits[:, -1, :] # Getting the logit values for the last token\n",
    "probabilities = torch.nn.functional.softmax(val[0], dim=-1) # Getting the probabilities values for the last token logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00e81ac5-31e2-4fbf-af42-47f40ece79a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the token id with maximum probability\n",
    "max_prob_token = probabilities.argmax()\n",
    "tokenizer.decode(max_prob_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5edb1dd-10d1-47ee-aa23-9e216be57b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4710</th>\n",
       "      <td>4710</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>0.227539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>576</td>\n",
       "      <td>The</td>\n",
       "      <td>0.088867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>758</td>\n",
       "      <td>In</td>\n",
       "      <td>0.064941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>1752</td>\n",
       "      <td>For</td>\n",
       "      <td>0.047607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>1205</td>\n",
       "      <td>We</td>\n",
       "      <td>0.034912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>715</td>\n",
       "      <td>\\n</td>\n",
       "      <td>0.034912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>5209</td>\n",
       "      <td>Please</td>\n",
       "      <td>0.034912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>1096</td>\n",
       "      <td>This</td>\n",
       "      <td>0.032715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>1084</td>\n",
       "      <td>It</td>\n",
       "      <td>0.028931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>1207</td>\n",
       "      <td>Q</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_id    token  probability\n",
       "4710      4710     \\n\\n     0.227539\n",
       "576        576      The     0.088867\n",
       "758        758       In     0.064941\n",
       "1752      1752      For     0.047607\n",
       "1205      1205       We     0.034912\n",
       "715        715       \\n     0.034912\n",
       "5209      5209   Please     0.034912\n",
       "1096      1096     This     0.032715\n",
       "1084      1084       It     0.028931\n",
       "1207      1207        Q     0.022461"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting all the probabilities\n",
    "prob_df = pd.DataFrame([(id_, tokenizer.decode(id_), prob_value.item()) for id_, prob_value in enumerate(probabilities)], \n",
    "                      columns=[\"token_id\", \"token\", \"probability\"])\n",
    "prob_df = prob_df.sort_values(by=\"probability\", ascending=False)\n",
    "prob_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1cde2-a6c4-4426-9284-3d57a46e3fa7",
   "metadata": {},
   "source": [
    "## Lets generate next 100 tokens by using generate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "309cd8f4-df48-4df4-9c7f-f0aa58ea4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `generate` method to generate lots of text\n",
    "output = model.generate(inputs, max_length=200, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a4c6c539-9211-478c-8442-5b699f20da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT \n",
      "\n",
      " Qwen2 is the new series of Qwen large language models. For Qwen2, we release a number of base language models and \n",
      "instruction-tuned language models ranging from 0.5 to 72 billion parameters, including a Mixture-of-Experts model. This repo contains the \n",
      "instruction-tuned 1.5B Qwen2 model.\n",
      "\n",
      "Compared with the state-of-the-art opensource language models, including the previous released Qwen1.5, Qwen2 has generally surpassed most \n",
      "opensource models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language \n",
      "understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc.\n",
      "==================================================================================================================================\n",
      "\n",
      "OUTPUT_TEXT \n",
      "\n",
      "Qwen2 is the new series of Qwen large language models. For Qwen2, we release a number of base language models and \n",
      "instruction-tuned language models ranging from 0.5 to 72 billion parameters, including a Mixture-of-Experts model. This repo contains the \n",
      "instruction-tuned 1.5B Qwen2 model.\n",
      "\n",
      "Compared with the state-of-the-art opensource language models, including the previous released Qwen1.5, Qwen2 has generally surpassed most \n",
      "opensource models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language \n",
      "understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc. \n",
      "\n",
      "The following are the key features:\n",
      "\n",
      "* **Language Understanding**: The model can understand natural language queries and generate appropriate answers.\n",
      "  * Example: \"What is the capital of France?\"\n",
      "  * Model output: \"Paris\"\n",
      "\n",
      "* **Natural Language Generation**: The model can generate text that matches the input\n"
     ]
    }
   ],
   "source": [
    "print(f\"INPUT TEXT \\n\\n {INPUT_TEXT}\", end=\"\\n\"+\"=\"*130+\"\\n\\n\")\n",
    "print(f\"OUTPUT_TEXT \\n\\n{tokenizer.decode(output[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c87e236-5430-4e3f-be9c-3b00b66a7297",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------- <br>\n",
    "So the output generated by using generate method also includes the input tokens and the number of tokens can be calculated by<br>\n",
    "\n",
    "number of new token generated = max_length value - number of input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d8885-bb05-4880-9ee4-c3ae89696d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e196c490-f799-473d-8a8a-678deb117b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-env)",
   "language": "python",
   "name": "llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
